{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWgYXQW1kuSg"
      },
      "source": [
        "# DTSA 5799 Unsupervised Text Classification for Marketing Analytics Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rfw11JBiZHi"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jpxYdJGQiWqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca5455c9-0f3d-4f2a-b3b8-4fe2094b38c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lda\n",
            "  Downloading lda-2.0.0-cp37-cp37m-manylinux1_x86_64.whl (351 kB)\n",
            "\u001b[K     |████████████████████████████████| 351 kB 15.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from lda) (1.21.6)\n",
            "Collecting pbr<4,>=0.6\n",
            "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pbr, lda\n",
            "Successfully installed lda-2.0.0 pbr-3.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tmtoolkit\n",
            "  Downloading tmtoolkit-0.10.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 15.5 MB/s \n",
            "\u001b[?25hCollecting spacy<2.4,>=2.3.0\n",
            "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 46.3 MB/s \n",
            "\u001b[?25hCollecting scipy<1.6,>=1.5.0\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 2.3 MB/s \n",
            "\u001b[?25hCollecting matplotlib<3.4,>=3.3.0\n",
            "  Downloading matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 45.9 MB/s \n",
            "\u001b[?25hCollecting xlrd>=1.2.0\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from tmtoolkit) (1.21.6)\n",
            "Collecting globre<0.2,>=0.1.5\n",
            "  Downloading globre-0.1.5.tar.gz (20 kB)\n",
            "Collecting pandas<1.2,>=1.1.0\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4,>=3.3.0->tmtoolkit) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4,>=3.3.0->tmtoolkit) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4,>=3.3.0->tmtoolkit) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4,>=3.3.0->tmtoolkit) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.4,>=3.3.0->tmtoolkit) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.4,>=3.3.0->tmtoolkit) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.2,>=1.1.0->tmtoolkit) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<3.4,>=3.3.0->tmtoolkit) (1.15.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (2.23.0)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (0.9.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (4.64.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4,>=2.3.0->tmtoolkit) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4,>=2.3.0->tmtoolkit) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4,>=2.3.0->tmtoolkit) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4,>=2.3.0->tmtoolkit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4,>=2.3.0->tmtoolkit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4,>=2.3.0->tmtoolkit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4,>=2.3.0->tmtoolkit) (2022.5.18.1)\n",
            "Building wheels for collected packages: globre\n",
            "  Building wheel for globre (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for globre: filename=globre-0.1.5-py3-none-any.whl size=19546 sha256=e104b9411ed57b5b077806c2abf5b71d76f51b9997b3e24639b62c7f5db776a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/95/37/5303ce04fce53b6e64ed74a3f0a0e9ea11c348cac0c5c42a76\n",
            "Successfully built globre\n",
            "Installing collected packages: thinc, xlrd, spacy, scipy, pandas, matplotlib, globre, tmtoolkit\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed globre-0.1.5 matplotlib-3.3.4 pandas-1.1.5 scipy-1.5.4 spacy-2.3.7 thinc-7.4.5 tmtoolkit-0.10.0 xlrd-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "    from tmtoolkit.corpus import Corpus\n",
        "    from tmtoolkit.preprocess import TMPreproc\n",
        "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
        "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
        "except ModuleNotFoundError:\n",
        "    !pip install lda\n",
        "    !pip install tmtoolkit\n",
        "    from tmtoolkit.corpus import Corpus\n",
        "    from tmtoolkit.preprocess import TMPreproc\n",
        "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
        "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8g9kSJwi7M7"
      },
      "source": [
        "## Implement a pre-processor\n",
        "\n",
        "Here you will implement a function called `preprocess` which returns the TMPreproc object to be used for topic modeling.\n",
        "\n",
        "The preprocess function will take a list of texts and return a pre-processed corpus object, i.e. a TMPreproc object. Preprocessing should include the following actions on the corpus using the appropriate methods in the TMPreproc class:\n",
        "\n",
        " - lemmatize the texts\n",
        " - convert tokens to lowercase\n",
        " - remove special characters\n",
        " - clean tokens to remove numbers and any tokens shorter than 3 characters\n",
        "\n",
        "The first part of the function to create the corpus and preprocess object are done for you. Your job is to call the specific preprocess functions and to return the resulting preprocess object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eQFevchKhohK"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts, lang=\"en\"):\n",
        "    \"\"\"Preprocessor which returns a TMPreproc object processed on corpus as language\n",
        "    specified by lang (defaults to \"en\"):\n",
        "\n",
        "    Should perform all of the following pre-processing functions:\n",
        "     - lemmatize\n",
        "     - tokens_to_lowercase\n",
        "     - remove_special_chars_in_tokens\n",
        "     - clean_tokens (remove numbers, and remove tokens shorter than 2)\n",
        "    \"\"\"\n",
        "    # Here, we just use the index of the text as the label for the corpus item\n",
        "    corpus = Corpus({ i:r for i, r in enumerate(texts) })\n",
        "\n",
        "    preproc = TMPreproc(corpus, language=lang)\n",
        "\n",
        "    TMPreproc.lemmatize(preproc)\n",
        "    TMPreproc.tokens_to_lowercase(preproc)\n",
        "    TMPreproc.remove_special_chars_in_tokens(preproc)\n",
        "    TMPreproc.clean_tokens(preproc, remove_shorter_than=3, remove_numbers=True)\n",
        "\n",
        "\n",
        "    return preproc\n",
        "\n",
        "    # TODO: Complete the implementation of this function and submit the\n",
        "    # .py download of this notebook as your assignment submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBlUJqEan3oc"
      },
      "source": [
        "## Function development\n",
        "\n",
        "Use this section of code to verify your function implementation. You may change the test_corpus as needed to verify your implementation. The grader will be checking that your function returns a TMPreproc object that meets all of the following critera:\n",
        "\n",
        " - tokens are lemmatized\n",
        " - tokens are converted to lowercase\n",
        " - special characters are removed from tokens\n",
        " - tokens shorter than 3 characters and numerics are removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YBlpS0RsrJ3s"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RmT0BazGQg"
      },
      "source": [
        "## Topic modeling Amazon Reviews\n",
        "\n",
        "Once you have completed the assignment above, you will be well prepared to start your final project for this unit. The project will include loading Amazon reviews into a corpus for topic modeling. The code below demonstrates topic modeling the reviews for a given brand. Note that the final project will require additional segmentation of the data, which is not done for you in the example here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpBeHmNwaMcW",
        "outputId": "355956e4-8f61-43a2-84b9-46acce1420e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RpcCkHX1P1d6"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "asins = []\n",
        "\n",
        "# To run this code, you will need to download the metadata file from the course\n",
        "# assets and upload it to your Google Drive. See the notes about that file\n",
        "# regarding how it was processed from the original file into json-l format.\n",
        "\n",
        "with gzip.open(\"drive/MyDrive/meta_Clothing_Shoes_and_Jewelry.jsonl.gz\") as products:\n",
        "    for product in products:\n",
        "        data = json.loads(product)\n",
        "        categories = [c.lower() for c in\n",
        "                      list(itertools.chain(*data.get(\"categories\", [])))]\n",
        "        if \"nike\" in categories:\n",
        "            asins.append(data[\"asin\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a corpus of review texts"
      ],
      "metadata": {
        "id": "y0bwBkHsVrsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u6HT0Yjy3gNG"
      },
      "outputs": [],
      "source": [
        "review_corpus = []\n",
        "with gzip.open(\"drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry.json.gz\") as reviews:\n",
        "    for review in reviews:\n",
        "        data = json.loads(review)\n",
        "        if data[\"asin\"] in asins:\n",
        "            text = data[\"reviewText\"]\n",
        "            review_corpus.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect a few of the reviews"
      ],
      "metadata": {
        "id": "hKrH89AKVwCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A9Ct4cj7Tih9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03132ae2-0285-4e65-d3fb-b3d58e84895f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 the colour i received is not blue as shown but yellow.Couldnt change it because \n",
            "1 Very cute and is really practical. Fits better on smaller wrists which is my cas\n",
            "2 The watch was exactly what i ordered and I got it very fast. Unfortunately it wa\n",
            "3 This product came promptly and as described, pleasure doing business with them!-\n",
            "4 Why isn't Nike making these anymore?  I love this watch, and I get a lot of comp\n"
          ]
        }
      ],
      "source": [
        "for i, review in enumerate(review_corpus[:5]):\n",
        "    print(i, review[:80])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a TMPreproc object from the review corpus"
      ],
      "metadata": {
        "id": "q9ppd90QVzgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "taIJ_BZU7E81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40baad30-6b72-4c68-f713-6d36cf693b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.7). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "pre = preprocess(review_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wq_-wpEE8cnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f54ab37-0933-4915-f204-a63c01439c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lda:n_documents: 21570\n",
            "INFO:lda:n_words: 460163\n",
            "INFO:lda:vocab_size: 18131\n",
            "INFO:lda:n_topics: 20\n",
            "INFO:lda:n_iter: 100\n",
            "WARNING:lda:all zero row in document-term matrix found\n",
            "INFO:lda:<0> log likelihood: -5256335\n",
            "INFO:lda:<10> log likelihood: -3803279\n",
            "INFO:lda:<20> log likelihood: -3649503\n",
            "INFO:lda:<30> log likelihood: -3573576\n",
            "INFO:lda:<40> log likelihood: -3532709\n",
            "INFO:lda:<50> log likelihood: -3504166\n",
            "INFO:lda:<60> log likelihood: -3484099\n",
            "INFO:lda:<70> log likelihood: -3468593\n",
            "INFO:lda:<80> log likelihood: -3459168\n",
            "INFO:lda:<90> log likelihood: -3451574\n",
            "INFO:lda:<99> log likelihood: -3444859\n"
          ]
        }
      ],
      "source": [
        "dtms = {\n",
        "    \"reviews_corpus\": pre.dtm\n",
        "}\n",
        "lda_params = {\n",
        "    'n_topics': 20,\n",
        "    'eta': .01,\n",
        "    'n_iter': 100,\n",
        "    'random_state': 777,  # to make results reproducible\n",
        "    'alpha': 1/16\n",
        "}\n",
        "\n",
        "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the topics"
      ],
      "metadata": {
        "id": "RGRrYqRuV7nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[\"reviews_corpus\"][0][1]\n",
        "print_ldamodel_topic_words(model.topic_word_, pre.vocabulary, top_n=5)"
      ],
      "metadata": {
        "id": "D8GYqMPHjVH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836b82db-6c6b-4850-e9e4-5e27fa689607"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_1\n",
            "> #1. color (0.060760)\n",
            "> #2. shoe (0.040493)\n",
            "> #3. look (0.032800)\n",
            "> #4. like (0.030566)\n",
            "> #5. black (0.027630)\n",
            "topic_2\n",
            "> #1. shoe (0.064983)\n",
            "> #2. foot (0.047672)\n",
            "> #3. size (0.035678)\n",
            "> #4. fit (0.028079)\n",
            "> #5. wear (0.023104)\n",
            "topic_3\n",
            "> #1. shoe (0.057409)\n",
            "> #2. play (0.037918)\n",
            "> #3. good (0.036248)\n",
            "> #4. basketball (0.027793)\n",
            "> #5. great (0.025313)\n",
            "topic_4\n",
            "> #1. order (0.031153)\n",
            "> #2. shoe (0.029330)\n",
            "> #3. ship (0.019902)\n",
            "> #4. size (0.018971)\n",
            "> #5. return (0.018389)\n",
            "topic_5\n",
            "> #1. run (0.040772)\n",
            "> #2. much (0.020806)\n",
            "> #3. good (0.019034)\n",
            "> #4. shirt (0.016748)\n",
            "> #5. light (0.015348)\n",
            "topic_6\n",
            "> #1. shoe (0.068555)\n",
            "> #2. love (0.042773)\n",
            "> #3. nike (0.034915)\n",
            "> #4. comfortable (0.033216)\n",
            "> #5. great (0.033173)\n",
            "topic_7\n",
            "> #1. sock (0.032171)\n",
            "> #2. wear (0.031252)\n",
            "> #3. get (0.029075)\n",
            "> #4. boot (0.023124)\n",
            "> #5. like (0.020367)\n",
            "topic_8\n",
            "> #1. shoe (0.034488)\n",
            "> #2. foot (0.031290)\n",
            "> #3. like (0.015112)\n",
            "> #4. good (0.014799)\n",
            "> #5. sole (0.014391)\n",
            "topic_9\n",
            "> #1. shoe (0.080829)\n",
            "> #2. run (0.036028)\n",
            "> #3. comfortable (0.034479)\n",
            "> #4. foot (0.034238)\n",
            "> #5. walk (0.029352)\n",
            "topic_10\n",
            "> #1. watch (0.091895)\n",
            "> #2. time (0.019909)\n",
            "> #3. band (0.015187)\n",
            "> #4. use (0.014130)\n",
            "> #5. work (0.011663)\n",
            "topic_11\n",
            "> #1. shoe (0.052865)\n",
            "> #2. nike (0.026066)\n",
            "> #3. buy (0.022321)\n",
            "> #4. pair (0.022137)\n",
            "> #5. much (0.020155)\n",
            "topic_12\n",
            "> #1. shoe (0.086863)\n",
            "> #2. run (0.042041)\n",
            "> #3. nike (0.031964)\n",
            "> #4. pair (0.025788)\n",
            "> #5. air (0.016578)\n",
            "topic_13\n",
            "> #1. shoe (0.029636)\n",
            "> #2. get (0.018274)\n",
            "> #3. make (0.018132)\n",
            "> #4. nike (0.014487)\n",
            "> #5. price (0.013587)\n",
            "topic_14\n",
            "> #1. pair (0.070905)\n",
            "> #2. buy (0.034821)\n",
            "> #3. wear (0.034315)\n",
            "> #4. find (0.028352)\n",
            "> #5. year (0.024157)\n",
            "topic_15\n",
            "> #1. shoe (0.070077)\n",
            "> #2. great (0.062281)\n",
            "> #3. fit (0.050806)\n",
            "> #4. good (0.044893)\n",
            "> #5. comfortable (0.043360)\n",
            "topic_16\n",
            "> #1. good (0.049886)\n",
            "> #2. shoe (0.032465)\n",
            "> #3. wear (0.031586)\n",
            "> #4. last (0.020265)\n",
            "> #5. buy (0.019903)\n",
            "topic_17\n",
            "> #1. bag (0.069697)\n",
            "> #2. gym (0.026674)\n",
            "> #3. muy (0.021768)\n",
            "> #4. pocket (0.016148)\n",
            "> #5. carry (0.014002)\n",
            "topic_18\n",
            "> #1. size (0.141293)\n",
            "> #2. small (0.052116)\n",
            "> #3. shoe (0.048308)\n",
            "> #4. order (0.043216)\n",
            "> #5. fit (0.034936)\n",
            "topic_19\n",
            "> #1. love (0.083960)\n",
            "> #2. shoe (0.035719)\n",
            "> #3. fit (0.035488)\n",
            "> #4. buy (0.031607)\n",
            "> #5. son (0.031468)\n",
            "topic_20\n",
            "> #1. good (0.063928)\n",
            "> #2. product (0.047847)\n",
            "> #3. fit (0.039158)\n",
            "> #4. great (0.038425)\n",
            "> #5. quality (0.035716)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RE_5cnylabod"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Topic_week6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}